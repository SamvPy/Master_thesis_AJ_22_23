{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StratifiedKFold vs Project split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# Models\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# preprocessors\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif, SelectKBest, SelectFromModel, RFE\n",
    "\n",
    "# Samplers\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# metrics and splitters\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# utils\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib_venn as venn\n",
    "import random\n",
    "\n",
    "# progress bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import utils_ML as uml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 2.6564102564102563, 7: 3.453333333333333, 9: 0.7194444444444444, 1: 0.3453333333333333, 0: 0.4427350427350427, 8: 1.5014492753623188, 14: 3.453333333333333, 6: 0.5755555555555556, 3: 2.3022222222222224, 11: 1.3282051282051281, 5: 1.4388888888888889, 12: 1.4388888888888889, 10: 1.3282051282051281, 4: 0.8222222222222222, 13: 1.817543859649123}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../PEMatrix/norm_NSAF_data2.csv\", index_col = \"assay_id\")\n",
    "meta = pd.read_csv(\"../../Metadata/unified_metadata.csv\")\n",
    "meta = meta[meta.assay_id.isin(data.index)]\n",
    "\n",
    "groups = pd.read_csv(\"../../Metadata/group_cells_annotation.csv\", sep =\";\", index_col=\"Unnamed: 0\")\n",
    "meta[\"Group\"] = meta.cell_line.apply(lambda x: groups[groups.cell_line == x][\"group\"].values[0])\n",
    "meta = meta.set_index(\"assay_id\")\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "targets = target_encoder.fit_transform(meta.Group)\n",
    "unique_labels = pd.Series(targets).unique()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=targets)\n",
    "\n",
    "weights = {unique_labels[i]: class_weights[i] for i in range(len(unique_labels))}\n",
    "print(weights)\n",
    "\n",
    "data.sort_index(inplace=True)\n",
    "meta.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index must be 0-n_samples bcz prob index is taken with iloc instead of loc\n",
    "class ProjectBasedSplit():\n",
    "    def __init__(self, splits: int, metadata: pd.DataFrame, on = str):\n",
    "        \"\"\"Called when training model and splitting_procedure is set as 'project'\n",
    "        \n",
    "        metadata: the metadata table that is used to generate splits\n",
    "        \n",
    "        on: the column name that represent the class column name\"\"\"\n",
    "\n",
    "        self.splits = splits\n",
    "        self.metadata = metadata.reset_index(drop=True)\n",
    "        self.label = on\n",
    "        self.label_indices = list(range(metadata[self.label].nunique()))\n",
    "        self.dropped_pxds = []\n",
    "\n",
    "    def split(self, dataset, metadata, groups = None):\n",
    "        \n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        metadata = self.metadata.loc[self.metadata.index.isin(dataset.index),:]\n",
    "\n",
    "        index_splits = []\n",
    "        for split in range(self.splits):\n",
    "            train_index, test_index, dropped_pxds = self.train_test_project_split(dataset, metadata=metadata)\n",
    "            index_splits.append((train_index, test_index))\n",
    "            self.dropped_pxds.append(dropped_pxds)\n",
    "\n",
    "            yield train_index, test_index\n",
    "        \n",
    "    def train_test_project_split(self, dataset, metadata: pd.DataFrame, groups = None):\n",
    "\n",
    "        indices = list(range(15))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        choosen_PXD = []\n",
    "\n",
    "        for group, PXD in self.metadata.groupby(self.label).PXD_accession.unique().iloc[indices].iteritems():\n",
    "            \n",
    "            if True in [pxd in choosen_PXD for pxd in PXD]:\n",
    "                continue\n",
    "            if len(PXD) > 1:\n",
    "                choosen_PXD.append(random.choice(PXD))\n",
    "            if self.metadata[~self.metadata.PXD_accession.isin(choosen_PXD)].groupby(self.label).PXD_accession.nunique().shape[0] != 15:\n",
    "                choosen_PXD = choosen_PXD[:-1]\n",
    "            if len(choosen_PXD) == 5:\n",
    "                break\n",
    "\n",
    "        test_index = self.metadata[self.metadata.PXD_accession.isin(choosen_PXD)].index\n",
    "        train_index = dataset.loc[~dataset.index.isin(test_index), :].index.to_numpy()\n",
    "\n",
    "        return train_index, test_index, choosen_PXD   \n",
    "\n",
    "    def get_n_splits(self, x, y, groups = None):\n",
    "        return self.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06ef5a857704ed69e53db6444f36c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = IntProgress(min=0, max= 5 * 6) \n",
    "display(f)\n",
    "\n",
    "splitter = ProjectBasedSplit(5, meta, on = \"Group\")\n",
    "\n",
    "fold=0\n",
    "for train, test in splitter.split(data, None):\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "    X_train = data.iloc[train,:]\n",
    "    Y_train = targets[train]\n",
    "    X_test = data.iloc[test,:]\n",
    "    Y_test = targets[test]\n",
    "\n",
    "    for filter_percentage in [.2, .4, .5, .6, .75, .9]:\n",
    "        filtering = uml.FilterByOccurence(percentage=filter_percentage)\n",
    "        imputer = uml.LowestValueImputer()\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        preprocessor = Pipeline(steps=[\n",
    "            ('filtering', filtering),\n",
    "            ('imputer', imputer),\n",
    "            ('scaler', scaler)\n",
    "        ])\n",
    "\n",
    "        # Preprocess the data\n",
    "        preprocessor.fit(X_train)\n",
    "        X_train_preprocessed = preprocessor.transform(X_train)\n",
    "        X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "        X_oversampled, Y_oversampled = SMOTETomek().fit_resample(X_train_preprocessed, Y_train)\n",
    "\n",
    "        model = LogisticRegression(max_iter= 10000)\n",
    "\n",
    "        model.fit(X_oversampled, Y_oversampled)\n",
    "        Y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "        micro_f1, macro_f1, weighted_f1, cm = uml.scoring_functions(Y_pred=Y_pred, Y_test=Y_test, labels=np.unique(Y_test))\n",
    "            \n",
    "        results_df = pd.DataFrame({\"model\": [type(model).__name__], \"fold\": [fold], \"micro_f1\": [micro_f1],\n",
    "                                        \"macro_f1\": [macro_f1], \"weighted_f1\": [weighted_f1] ,\"cm\": [cm], \"filter_type\": [\"global\"], 'projects': [splitter.dropped_pxds[fold-1]],\n",
    "                                        \"filter_percentage\": [filter_percentage], \"proteins\": [len(preprocessor.named_steps.filtering.filtered_proteins)]})\n",
    "            \n",
    "        uml.save_results(results_df, \"global_filtering_pxdsplit\")\n",
    "    \n",
    "        f.value += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ionbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
